{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S15_QTable_Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSA--2K8L4IQ"
      },
      "source": [
        "# Giới thiệu Reinforcement learning (Rl) - learning to act\n",
        "\n",
        "Reinforcement learning - học tăng cường được định nghĩa một cách tổng quan nhất là các bài toán trong đó mục tiêu là để tối đa hóa phần thưởng trong dài hạn. Ví dụ khi chơi cờ, điều mà người chơi có kinh nghiệm phải tính toán là khi đi mỗi nước cờ hoặc cụm các nước cờ bằng cách nào đấy đều phải đóng góp đến kết quả toàn cục.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cf/CaroVN.jpg\" height=\"270\">\n",
        "</center>\n",
        "\n",
        "Mô hình của DQN gồm *tác nhân (agent)* tương tác với *môi trường (environment)* tại *trạng thái (state)* thông qua các *hành động (action)* tại thời điểm $t$ và nhận về *phần thưởng (reward)*. Sau mỗi bước hành động tác nhân sẽ chuyển tiếp sang trạng thái mới. Một vòng lặp các phản ứng qua lại giữa tác nhân - môi trường được mô phỏng như hình sau:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/904/1*WOYVzYnF-rbdcgZU2Wt9Yw.png\" height=\"200\">\n",
        "</center>\n",
        "\n",
        "Trong đó:\n",
        "\n",
        "- Trạng thái thể hiện tình trạng của tác nhân và môi trường tại thời điểm $t$.\n",
        "\n",
        "- Tập hợp bộ hành động tác nhân có thể làm tại mỗi trạng thái.\n",
        "\n",
        "- Tập hợp bộ phần thưởng cho từng trạng thái - hành động.\n",
        "\n",
        "***Tổng quan là:***\n",
        "\n",
        "> *Với trạng thái hiện tại, bộ hành động và tập hợp các phần thưởng tương ứng đã được quy định trước, máy sẽ học cách chọn ra hành động để tối đa hóa phần thưởng nhận được trong tương lai.*\n",
        "\n",
        "Vậy chắc chắn là ở đây đang tồn tại 1 cái hàm nào đó có đầu vào đầu ra. Đầu vào bao gồm các trạng thái, bộ action có thể thực hiện tại từng trạng thái, reward cho mỗi cặp trạng thái - hành động. Còn đầu ra là hành động nên làm.\n",
        "\n",
        "Và cái hàm mà nhận vào 1 bảng (ta gọi là bảng *Q-value*) và trả ra hành động tương ứng gọi là *hàm chiến lược (policy)*, ký hiệu là $\\pi$. \n",
        "\n",
        "***Như vậy:***\n",
        "\n",
        "> *Thuật toán sẽ đi tìm hàm chiến lược tối ưu kia. Tối ưu dựa trên tiêu chí là, nó sẽ học cách tính toán hành động nào nên làm để tối đa phần thưởng nhiều nhất có thể trong tương lai.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtQT5ZfFbl_j"
      },
      "source": [
        "# Phân tích theo TEFPA\n",
        "\n",
        "- Task: input là state, output là optimal action.\n",
        "\n",
        "- E: \n",
        "  + Input: trạng thái hiện tại, hành động kế tiếp\n",
        "  + Output: phần thưởng ở hiện tại.\n",
        "\n",
        "- F: hàm chiến lược $\\pi$. \n",
        "\n",
        "- P: Phương trình Bellman:\n",
        "\n",
        "$$\n",
        "Q^{target}(s_t,a_t) \\approx r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1}))\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "P = \\Delta{Q} = \\hat{Q}(s_t,a_t) - Q^{target}(s_t,a_t)\n",
        "$$\n",
        "\n",
        "- A:\n",
        "\n",
        "$$\n",
        "\\hat{Q}(s_t,a_t) = \\hat{Q}(s_t,a_t) - \\eta*\\Delta{Q} \\\\\n",
        "= \\hat{Q}(s_t,a_t) +\\eta[r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1})) - \\hat{Q}(s_t,a_t)]\n",
        "$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbq6OVRuuIh9"
      },
      "source": [
        "# Code với game Hồ băng\n",
        "\n",
        "Mô tả: Game hồ băng, chia làm 16 ô vuông gồm ô bắt đầu, các ô vuông có băng hoặc không băng và ô đích. Ô bắt đầu chú thích là `S`, ô có băng chú thích là `F`, ô là một cái hố chú thích là `H`, ô đích đến chú thích là `G`. Nhiệm vụ của bạn là di chuyển từ `S` đến `G`. Nếu đi lọt vào ô hố thì game over!\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://miro.medium.com/max/884/1*MCjDzR-wfMMkS0rPqXSmKw.png)\n",
        "<center>\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/842/1*Qp14HWQfOeE2UoSxrxCxAg.png\" height=\"270\">\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "### Q-values table\n",
        "\n",
        "|State(cell)|Left|Down|Right|Up|\n",
        "|---|---|---|---|---|\n",
        "|00 | 0 | 0 | 0 | 0 |\n",
        "|01 | 0 | 0 | 0 | 0 |\n",
        "|...| 0 | 0 | 0 | 0 |\n",
        "|15 | 0 | 0 | 0 | 0 |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1KI19m2oWz0"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import gym # tạo sẵn mỗi trường game cho Decision Making: taxi, Frozen Lake, Mario \n",
        "import random\n",
        "from gym.envs.registration import register\n",
        "np.random.seed(1612)\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.table import Table\n",
        "\n",
        "## Load and setup the environment\n",
        "env = gym.make('FrozenLake-v0', is_slippery=False) # ko có trượt, đi bước nào đúng bước đó\n",
        "#To start, we need to reset to the initial (default) state\n",
        "env.reset()\n",
        "#Get the total number of possibles states (obervations)\n",
        "states = env.observation_space.n # .n để chuyển sang data dạng số \n",
        "#Get the total number of actions available to the agent\n",
        "actions = env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR7b4S9synTv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "592b4f4a-6f19-4cab-a5d2-35c385cb68d3"
      },
      "source": [
        "#To visualize the current state\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr3rqsfCykum",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "15164570-abd0-4e51-cd84-de5639b31866"
      },
      "source": [
        "## Initialize Q-table structure\n",
        "#---->TODO<----: create a table to contain expected values at given states\n",
        "Q = np.zeros([states,actions]) # ngoặc [] or () bên trong thì nó tự hiểu mình quy định kiểu numpy array\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTqLqnqA9aYs"
      },
      "source": [
        "## For Q-learning\n",
        "# Hyperparameters: \n",
        "epsilon = 0.99 #For the epsilon-greedy approach\n",
        "gamma = 0.9 #Discount factor\n",
        "eta = 0.8 #Learning rate\n",
        "#The maximum amount of times we'll run the game\n",
        "total_episodes = 1000\n",
        "\n",
        "#The maximum steps we'll run for every episode \n",
        "max_steps = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpKR6Ex5jzqg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6fefd0cc-2937-4e4d-bd85-1dad640bb0ca"
      },
      "source": [
        "np.random.uniform(0,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0809997412036868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqF28sHGrCjl"
      },
      "source": [
        "$$\n",
        "Q^{target}(s_t,a_t) \\approx r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1}))\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "P = \\Delta{Q} = \\hat{Q}(s_t,a_t) - Q^{target}(s_t,a_t)\n",
        "$$\n",
        "\n",
        "- A:\n",
        "\n",
        "$$\n",
        "\\hat{Q}(s_t,a_t) = \\hat{Q}(s_t,a_t) - \\eta*\\Delta{Q} \\\\\n",
        "= \\hat{Q}(s_t,a_t) +\\eta[r_t + \\gamma max_{a_{t+1}}(Q(s_{t+1},a_{t+1})) - \\hat{Q}(s_t,a_t)]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRlxKxIhyf1a"
      },
      "source": [
        "#---->TODO:\n",
        "def choose_action(Q,state, epsilon):\n",
        "    #Randomly generate a number between 0 and 1 and see if it's smaller than epsilon\n",
        "    if np.random.uniform(0,1) < epsilon: # epsilon: thử nghiệm or khai phá : explore (mất thời gian chơi)\n",
        "    #If smaller, a random action is chosen using env.action_space.sample()\n",
        "      return env.action_space.sample()\n",
        "    #If greater, we choose the action having the maximum value in the Q-table for that state: a.k.a best action\n",
        "    return np.argmax(Q[state, :]) # epsilon: khai phá: exploit ---> tại state đó, act nào lớn nhất thì lấy\n",
        "\n",
        "#---->TODO: code Q-learning based on Q formula\n",
        "#input: old_value: Q[s,a] \n",
        "#       max_value_of_next_state: max(a') Q[s',a'] \n",
        "#output: new_value: Q[s,a] updated\n",
        "def update_Q(old_value, max_value_of_next_state, reward):\n",
        "    Q_target = reward + gamma * (max_value_of_next_state)\n",
        "    new_value = old_value - eta * (old_value - Q_target)\n",
        "    return new_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J55TpUgw45bz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "4be071b6-bb36-4b41-96c5-1b77fb353dcc"
      },
      "source": [
        "## Start\n",
        "rList = []\n",
        "for episode in range(total_episodes):\n",
        "    #Store the initial state using env.reset()\n",
        "    state = env.reset()\n",
        "    #Store the number of steps\n",
        "    t = 0\n",
        "    #Reduce epsilon gradually\n",
        "    epsilon = max(0.01, epsilon-0.001) # sau mỗi lần chơi trừ dần epsilon đi để bớt explore\n",
        "\n",
        "    rAll = 0\n",
        "    while t < max_steps:\n",
        "        # env.render()\n",
        "        #---->TODO: Choose appropriate action\n",
        "        action = choose_action(Q, state, epsilon)\n",
        "        #After the action taken in the environment, the reward for that action and next_state is returned\n",
        "        \n",
        "        #env.step() returns a tuple\n",
        "        #done (bool type) returns True if the espisode is finished\n",
        "        #info (dict type) stores the extra information for debugging purposes ---> ko dùng tới\n",
        "        next_state, reward, done, info = env.step(action) # .step tương tác với env (0, 1, 2, 3), next_step đã chơi rồi. Bài cũ ko có code này thì ko có tương tác nên giả lập bằng cách chèn next_step\n",
        "        #---->TODO: Append reward into rList # Q Learning là bảng kì vọng\n",
        "        rList.append(reward)\n",
        "\n",
        "        #Update Q-table \n",
        "        old_value = Q[state, action]\n",
        "        max_value_of_next_state = np.max(Q[next_state, :]) # từ dòng 21 do môi trường trả ra next_state # lấy max value chứ ko phải argmax value\n",
        "        #---->TODO: Update Q-value by calling th function update_Q\n",
        "        Q[state, action] = update_Q(old_value, max_value_of_next_state, reward)\n",
        "\n",
        "        #Set the current state as the next state   \n",
        "        state = next_state\n",
        "        t += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "    rList.append(rAll)\n",
        "    # print(Q, episode, epsilon)\n",
        "print(Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.531441 0.59049  0.59049  0.531441]\n",
            " [0.531441 0.       0.6561   0.59049 ]\n",
            " [0.59049  0.729    0.59049  0.6561  ]\n",
            " [0.6561   0.       0.59049  0.59049 ]\n",
            " [0.59049  0.6561   0.       0.531441]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.       0.81     0.       0.6561  ]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.6561   0.       0.729    0.59049 ]\n",
            " [0.6561   0.81     0.81     0.      ]\n",
            " [0.729    0.9      0.       0.729   ]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.       0.       0.       0.      ]\n",
            " [0.       0.81     0.9      0.729   ]\n",
            " [0.81     0.9      1.       0.81    ]\n",
            " [0.       0.       0.       0.      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2udi6Vv4Vvn"
      },
      "source": [
        "def play_one_game(env, policy):\n",
        "  cur_state = env.reset()\n",
        "  print('Starting position')\n",
        "  env.render()\n",
        "  while True:\n",
        "    best_action = policy[cur_state]\n",
        "    cur_state, reward, terminated, _ = env.step(best_action)\n",
        "    env.render()\n",
        "    if terminated and reward == 1:\n",
        "      print('You Win!')\n",
        "      break\n",
        "    elif terminated and reward == 0:\n",
        "      print('You Lose!')\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ918LMr4V3O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "d860385c-258c-4776-91fe-e71e453d39d9"
      },
      "source": [
        "play_one_game(env, policy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting position\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPtXVgkYyVjk"
      },
      "source": [
        "def visualize_policy(policy, terminal):\n",
        "  index_to_action = {\n",
        "    0:'left',\n",
        "    1:'down',\n",
        "    2:'right',\n",
        "    3:'up'\n",
        "  }\n",
        "  temp = []\n",
        "  for i in range(4):\n",
        "    temp_1 = []\n",
        "    for j in range(4):\n",
        "      temp_1.append(index_to_action.get(policy[i*4+j])) # ???\n",
        "    temp.append(temp_1)\n",
        "\n",
        "  data = pd.DataFrame(temp)\n",
        "  fig, ax = plt.subplots(figsize=(8,8))\n",
        "  ax.set_axis_off()\n",
        "  tb = Table(ax, bbox=[0,0,1,1])\n",
        "\n",
        "  nrows, ncols = 4, 4\n",
        "  width, height = 1.0 / ncols, 1.0 / nrows\n",
        "\n",
        "  bkg_colors = ['teal' for i in range(16)]\n",
        "  for pos in terminal:\n",
        "    bkg_colors[pos] = 'blue'\n",
        "  bkg_colors[0] = 'orange'\n",
        "  bkg_colors[15] = 'green'\n",
        "  bkg_colors\n",
        "\n",
        "  # Add cells\n",
        "  for (i,j), val in np.ndenumerate(data):\n",
        "    # Index either the first or second item of bkg_colors based on\n",
        "    # a checker board pattern\n",
        "    color = bkg_colors[i*4 + j]\n",
        "    if i*4+j in terminal and i*4+j!=15: # !!! \n",
        "      tb.add_cell(i, j, width, height, text='die', \n",
        "                  loc='center', facecolor=color)\n",
        "    elif i*4+j==15:\n",
        "      tb.add_cell(i, j, width, height, text='goal', \n",
        "                  loc='center', facecolor=color)\n",
        "    else:\n",
        "      tb.add_cell(i, j, width, height, text=val, \n",
        "                  loc='center', facecolor=color)\n",
        "\n",
        "  ax.add_table(tb)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8yrEmwWzc8F"
      },
      "source": [
        "def planning(env, iters, Q, gamma, terminal):\n",
        "  for i in range(iters):\n",
        "    for state in range(Q.shape[0]):\n",
        "      if state in terminal:\n",
        "        continue\n",
        "      for action in range(Q.shape[1]):\n",
        "        Q[state, action] = 0 # label like that bcuz Q will be implemented as zeros shape (16,4) as below\n",
        "        for probability, next_state, reward, terminated in env.P[state][action]:\n",
        "          Q[state, action] += probability * (reward + gamma * np.max(Q[next_state])) # Bellman as below or as here: https://hackmd.io/@COTAI/RLPlanning\n",
        "  return Q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkveBbk-yelb"
      },
      "source": [
        "def get_policy_from_Q(env, Q):\n",
        "  policy = np.zeros(shape=env.env.nS)\n",
        "  ## YOUR CODE GOES HERE ##\n",
        "  for state in range(Q.shape[0]): # no need to mention if state in terminal bcuz its wrong !!\n",
        "    policy[state] = np.argmax(Q[state]) \n",
        "  ########################\n",
        "  return policy.astype(np.uint8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6jEt5Ca5SSm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a552c88e-9a8b-4e6c-d89f-28242f42a3b9"
      },
      "source": [
        "## Visualize the movements\n",
        "\n",
        "#---->TODO: get a list of best actions manually\n",
        "import pandas as pd\n",
        "actions = [\"left\", \"down\", \"right\", \"up\"]\n",
        "dict_Q = {actions[0]: Q.T[0], actions[1]: Q.T[1], actions[2]: Q.T[2], actions[3]: Q.T[3]}  # visualize Q to easily understand\n",
        "print(pd.DataFrame(dict_Q),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        left     down    right        up\n",
            "0   0.531441  0.59049  0.59049  0.531441\n",
            "1   0.531441  0.00000  0.65610  0.590490\n",
            "2   0.590490  0.72900  0.59049  0.656100\n",
            "3   0.656100  0.00000  0.59049  0.590490\n",
            "4   0.590490  0.65610  0.00000  0.531441\n",
            "5   0.000000  0.00000  0.00000  0.000000\n",
            "6   0.000000  0.81000  0.00000  0.656100\n",
            "7   0.000000  0.00000  0.00000  0.000000\n",
            "8   0.656100  0.00000  0.72900  0.590490\n",
            "9   0.656100  0.81000  0.81000  0.000000\n",
            "10  0.729000  0.90000  0.00000  0.729000\n",
            "11  0.000000  0.00000  0.00000  0.000000\n",
            "12  0.000000  0.00000  0.00000  0.000000\n",
            "13  0.000000  0.81000  0.90000  0.729000\n",
            "14  0.810000  0.90000  1.00000  0.810000\n",
            "15  0.000000  0.00000  0.00000  0.000000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkikAiZtzCOY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a9e9040c-4665-4071-d958-5aea374230ef"
      },
      "source": [
        "terminal = [5, 7, 11, 12, 15]\n",
        "Q = np.zeros(shape=(env.env.nS, env.env.nA))\n",
        "\n",
        "# YOUR CODE GOES HERE\n",
        "\n",
        "Q = planning(env, 1000, Q, 0.8, terminal) # 100 epochs, 0.9 \n",
        "print(Q.shape)\n",
        "policy = get_policy_from_Q(env, Q)\n",
        "print(policy.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 4)\n",
            "(16,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7yU4dKLzYU_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "7beab591-dd92-4e6e-b0c4-78e00ef5b749"
      },
      "source": [
        "#---->TODO: visualize\n",
        "visualize_policy(policy, terminal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dJ6Yw1Czg68"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}